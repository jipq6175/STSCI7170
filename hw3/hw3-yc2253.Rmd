---
title: "STSCI 7170 Homework 3"
author: "Yen-Lin Chen (yc2253@cornell.edu)"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = TRUE,
	warning = TRUE
)
```


## Problem 1


### 1a

Let $Y_i = X_i^2$ and $Y = \sum_{i=1}^n Y_i$. If we can find the mgf of $Y_i$, $m_{Y_i}(t)$, the mgf of $Y$ is simply $m_{Y}(t) = \prod_{i=1}^nm_{Y_i}(t)$. 

$$m_{Y_i}(t) = E\left[e^{tY_i}\right] = E\left[e^{tX_i^2}\right] = \int e^{tx^2}\frac{1}{\sqrt{2\pi}}e^{-(x-\mu_i)^2/2}dx = \frac{1}{\sqrt{2\pi}} \int \text{exp}\left[ \frac{-1}{2}\left((1-2t)x^2-2\mu_ix \right)-\frac{1}{2}\mu_i^2\right]dx$$ 
$$ = \frac{1}{\sqrt{2\pi}}\int \text{exp}\left[ \frac{-1}{2}(1-2t)\left(x - \frac{\mu_i}{1-2t} \right)^2\right]\text{exp}\left(\frac{t\mu_i^2}{1-2t} \right)dx = \frac{1}{\sqrt{2\pi}}\text{exp}\left(\frac{t\mu_i^2}{1-2t} \right)\sqrt{\frac{2\pi}{1-2t}}$$
$$m_{Y_i}(t) = (1-2t)^{-1/2}\text{exp}\left(\frac{t\mu_i^2}{1-2t} \right)$$
$$m_{Y}(t) = \prod_{i=1}^n m_{Y_i}(t) = (1-2t)^{-n/2}\text{exp}\left(\frac{t\delta}{1-2t} \right) \quad \delta = \sum_{i=1}^n\mu_i^2 \quad t < \frac{1}{2}$$


### 1b

We first show that $f_{Y}(y, \delta)$ is a proper pdf and then $E[e^{tY}]$ is the same as the result in 1a.

$$\int_0^\infty f_{Y}(y, \delta)dy = \int_0^\infty \sum_{k=0}^\infty \frac{(\frac{\delta}{2})^ke^{-\delta/2}}{k!} \frac{y^{(n+2k)/2}e^{-y/2}}{\Gamma[(n+2k)/2]2^{(n+2k)/2}}$$
$$ = \sum_{k=0}^\infty \frac{(\frac{\delta}{2})^ke^{-\delta/2}}{k!} \frac{1}{\Gamma[(n+2k)/2]2^{(n+2k)/2}}\int_0^\infty y^{(n+2k)/2}e^{-y/2}dy$$
$$ = \sum_{k=0}^\infty \frac{(\frac{\delta}{2})^ke^{-\delta/2}}{k!} \frac{1}{\Gamma[(n+2k)/2]2^{(n+2k)/2}}\Gamma[(n+2k)/2]2^{(n+2k)/2}$$
$$ = \sum_{k=0}^\infty \frac{(\frac{\delta}{2})^ke^{-\delta/2}}{k!} = e^{\delta/2}e^{-\delta/2}=1$$

Moreover, 

$$E[e^{tY}] = \int_0^\infty \sum_{k=0}^\infty \frac{(\frac{\delta}{2})^ke^{-\delta/2}}{k!} \frac{y^{(n+2k)/2}e^{-(\frac{1}{2}-t)y}}{\Gamma[(n+2k)/2]2^{(n+2k)/2}} \quad t<\frac{1}{2}$$
$$ = \sum_{k=0}^\infty \frac{(\frac{\delta}{2})^ke^{-\delta/2}}{k!} \frac{1}{\Gamma[(n+2k)/2]2^{(n+2k)/2}}\frac{\Gamma[(n+2k)/2]}{(\frac{1}{2}-t)^{(n+2k)/2}}$$
$$ = \sum_{k=0}^\infty \frac{(\frac{\delta}{2})^ke^{-\delta/2}}{k!} \frac{1}{(1-2t)^{\frac{n}{2}+k}} = e^{-\delta/2}(1-2t)^{-n/2} \sum_{k=0}^\infty\frac{1}{k!}\left( \frac{\delta}{2(1-2t)}\right)^k$$
$$ = e^{-\delta/2}(1-2t)^{-n/2}e^{\frac{\delta}{2(1-2t)}} = (1-2t)^{-n/2}\text{exp}\left(\frac{t\delta}{1-2t} \right)$$

Therefore, $f_{Y}(y, \delta)$ is the pdf for the variable $Y = \sum_{i=1}^n X_i^2$.



## Problem 2

$$Y_{ijk} = \mu_0 + S_i+D_j+SD_{ij}+T_k+ST_{ik} +DT_{jk}+E_{ijk}$$
where $i \in \{1,2,\dots,n=5\}$, $j \in \{1, d=2 \}$ and $k \in \{1,2,3,t=4\}$. The subject is the random factor. We can rewrite the model as 

$$Y = \mu_0(\mathbf{1}_n\otimes \mathbf{1}_d\otimes\mathbf{1}_t)+(I_n\otimes \mathbf{1}_d\otimes\mathbf{1}_t)\tilde{S}+(\mathbf{1}_n\otimes I_d\otimes\mathbf{1}_t)\tilde{D}+(I_n\otimes I_d\otimes\mathbf{1}_t)\tilde{SD}$$

$$+ (\mathbf{1}_n\otimes \mathbf{1}_d\otimes I_t)\tilde{T} + (I_n\otimes \mathbf{1}_d\otimes I_t)\tilde{ST} + (\mathbf{1}_n\otimes I_d\otimes I_t)\tilde{DT} + (I_n \otimes I_d \otimes I_t)\tilde{E}$$
where $\tilde{S} \sim N_n(0, \sigma_S^2I_n)$, $\tilde{SD} \sim N_{nd}(0, \sigma_{SD}^2I_{nd})$, $\tilde{ST} \sim N_{nt}(0, \sigma_{ST}^2I_{nt})$ and $\tilde{E} \sim N_{ndt}(0, \sigma_E^2I_{ndt})$. 

The A-matrices are: 

1. $\mu_0$: $A_1 = \bar{J}_n \otimes \bar{J}_d \otimes \bar{J}_t = A_1^f$, $\text{rank}(A_1) = 1$
2. $\tilde{S}$: $A_2 = C_n \otimes \bar{J}_d \otimes \bar{J}_t = A_1^r$, $\text{rank}(A_2) = n-1$
3. $\tilde{D}$: $A_3 = \bar{J}_n \otimes C_d \otimes \bar{J}_t = A_2^f$, $\text{rank}(A_3) = d-1$
4. $\tilde{SD}$: $A_4 = C_n \otimes C_d \otimes \bar{J}_t = A_2^r$, $\text{rank}(A_4) = (n-1)(d-1)$
5. $\tilde{T}$: $A_5 = \bar{J}_n \otimes \bar{J}_d \otimes C_t = A_3^f$, $\text{rank}(A_5) = t-1$
6. $\tilde{ST}$: $A_6 = C_n \otimes \bar{J}_d \otimes C_t = A_3^r$, $\text{rank}(A_6) = (n-1)(t-1)$
7. $\tilde{DT}$: $A_7 = \bar{J}_n \otimes C_d \otimes C_t = A_4^f$, $\text{rank}(A_7) = (d-1)(t-1)$
8. $\tilde{E}$: $A_7 = C_n \otimes C_d \otimes C_t = A_4^r$, $\text{rank}(A_8) = (n-1)(d-1)(t-1)$

Let $Z_i = A_i^f+A_i^r$. 

$$I_{ndt} = \sum_{i=1}^8A_i = \sum_{i=1}^4 \left(A_i^r+A_i^f\right) = \sum_{i=1}^4Z_i$$
$$\Sigma = dt\sigma_S^2(I_n\otimes \bar{J}_d\otimes \bar{J}_t) + t\sigma_{SD}^2(I_n\otimes I_d\otimes \bar{J}_t)+d\sigma_{ST}^2(I_n\otimes \bar{J}_d\otimes I_t) + \sigma_E^2(I_n\otimes I_d\otimes I_t)$$

$$ = \left(dt\sigma_S^2+ t\sigma_{SD}^2 + d\sigma_{ST}^2+\sigma_E^2\right)(A_1+A_2)+\left( t\sigma_{SD}^2+\sigma_E^2 \right)(A_3+A_4)+\left(d\sigma_{ST}^2+\sigma_E^2 \right)(A_5+A_6)+\sigma_E^2(A_7+A_8)$$


$$ \Sigma = \left(dt\sigma_S^2+ t\sigma_{SD}^2 + d\sigma_{ST}^2+\sigma_E^2\right)Z_1+\left( t\sigma_{SD}^2+\sigma_E^2 \right)Z_2+\left(d\sigma_{ST}^2+\sigma_E^2 \right)Z_3+\sigma_E^2Z_4$$

$$\mu = E[Y] = Y = \mu_0(\mathbf{1}_n\otimes \mathbf{1}_d\otimes\mathbf{1}_t)+(\mathbf{1}_n\otimes I_d\otimes\mathbf{1}_t)\tilde{D}+(\mathbf{1}_n\otimes \mathbf{1}_d\otimes I_t)\tilde{T} + (\mathbf{1}_n\otimes I_d\otimes I_t)\tilde{DT}$$



### 2a

Under this setup, the expected mean squares (EMS) is defined as

$$\text{EMS}_i = \frac{Y'A_iY}{\text{rank}(A_i)}= c_i + \frac{\mu'A_i\mu}{\text{rank}(A_i)}$$

$$ \text{EMS}_2 = \frac{Y'A_2Y}{n-1} = dt\sigma_S^2+ t\sigma_{SD}^2 + d\sigma_{ST}^2+\sigma_E^2  \quad \because\mu'A_2\mu=0$$
$$ \text{EMS}_4 = \frac{Y'A_4Y}{(n-1)(d-1)} = t\sigma_{SD}^2+\sigma_E^2  \quad \because\mu'A_4\mu=0$$
$$\text{EMS}_6 = \frac{Y'A_6Y}{(n-1)(t-1)} = d\sigma_{ST}^2+\sigma_E^2  \quad \because\mu'A_6\mu=0$$
$$\text{EMS}_8 = \frac{Y'A_8Y}{(n-1)(d-1)(t-1)} = \sigma_E^2  \quad \because\mu'A_8\mu=0$$
$$\text{EMS}(\sigma_{E}^2) = \frac{Y'A_8Y}{(n-1)(d-1)(t-1)}$$
$$\text{EMS}(\sigma_{ST}^2) = \frac{1}{d}\left[\frac{Y'A_6Y}{(n-1)(t-1)} - \frac{Y'A_8Y}{(n-1)(d-1)(t-1)} \right]$$

$$\text{EMS}(\sigma_{SD}^2) = \frac{1}{t}\left[\frac{Y'A_4Y}{(n-1)(d-1)} - \frac{Y'A_8Y}{(n-1)(d-1)(t-1)} \right]$$

$$\text{EMS}(\sigma_S^2) = \frac{1}{dt}\left[ \frac{Y'A_2Y}{n-1} -\frac{Y'A_4Y}{(n-1)(d-1)} - \frac{Y'A_6Y}{(n-1)(t-1)} + \frac{Y'A_8Y}{(n-1)(d-1)(t-1)} \right]$$


The maximum likelihood (ML) estimate is defined as

$$\text{ML}_i = \frac{Y'A_i^rY}{\text{rank}(Z_i)} = \frac{c_i\text{rank}(A_i^r)}{\text{rank}(Z_i)} + \frac{\mu'A_i^r\mu}{\text{rank}(Z_i)} = \frac{\text{EMS}_i\text{rank}(A_i^r)}{\text{rank}(Z_i)}$$

$$\text{ML}_1 = \frac{Y'A_1^rY}{n} = \frac{n-1}{n}\left(dt\sigma_S^2+ t\sigma_{SD}^2 + d\sigma_{ST}^2+\sigma_E^2 \right)$$

$$ \text{ML}_2 = \frac{Y'A_2^rY}{n(d-1)} =  \frac{n-1}{n}\left(t\sigma_{SD}^2+\sigma_E^2 \right)$$

$$\text{ML}_3 = \frac{Y'A_3^rY}{n(t-1)}= \frac{n-1}{n}\left( d\sigma_{ST}^2+\sigma_E^2\right)$$

$$ \text{ML}_4 = \frac{Y'A_4^rY}{n(d-1)(t-1)} = \frac{n-1}{n}\sigma_E^2$$
$$\text{ML}(\sigma_{E}^2) = \frac{Y'A_4^rY}{(n-1)(d-1)(t-1)} = \text{EMS}(\sigma_{E}^2)$$

$$\text{ML}(\sigma_{ST}^2) = \frac{1}{d}\left[\frac{Y'A_3^rY}{(n-1)(t-1)} - \frac{Y'A_4^rY}{(n-1)(d-1)(t-1)} \right] = \text{EMS}(\sigma_{ST}^2)$$

$$\text{ML}(\sigma_S^2) = \frac{1}{t}\left[\frac{Y'A_2^rY}{(n-1)(d-1)} - \frac{Y'A_4^rY}{(n-1)(d-1)(t-1)} \right] = \text{EMS}(\sigma_S^2)$$

$$\text{ML}(\sigma_S^2) = \frac{1}{dt}\left[ \frac{Y'A_1^rY}{n-1} -\frac{Y'A_2^rY}{(n-1)(d-1)} - \frac{Y'A_3^rY}{(n-1)(t-1)} + \frac{Y'A_4^rY}{(n-1)(d-1)(t-1)} \right] = \text{EMS}(\sigma_S^2)$$

### 2b

Construct the $A$ matrices. 

```r
n = 4;
d = 2;
t = 5;

Jn = rep(1, n) %*% t(rep(1, n)) / n
Cn = diag(n) - Jn
Jd = rep(1, d) %*% t(rep(1, d)) / d
Cd = diag(d) - Jd
Jt = rep(1, t) %*% t(rep(1, t)) / t
Ct = diag(t) - Jt

A1 = Jn %x% Jd %x% Jt
A2 = Cn %x% Jd %x% Jt
A3 = Jn %x% Cd %x% Jt
A4 = Cn %x% Cd %x% Jt
A5 = Jn %x% Jd %x% Ct
A6 = Cn %x% Jd %x% Ct
A7 = Jn %x% Cd %x% Ct
A8 = Cn %x% Cd %x% Ct
```





## Moser 5.2

### a

$\hat{\beta} = (X'X)^{-1}X'Y$ is an unbiased estimator for $\beta$ because

$$E[\hat{\beta}] = (X'X)^{-1}X'E[Y] = (X'X)^{-1}X'X\beta = \beta$$

### b

Since $VX = XF$ where $F$ is non-singular, we have $X'V = F'X'$ and $X' = F'X'V^{-1}$. Therefore, 
$$\hat{\beta} = (X'X)^{-1}X'Y = (F'X'V^{-1}X)^{-1}F'X'V^{-1}Y = (X'V^{-1}X)^{-1}(F')^{-1}F'X'V^{-1}Y = (X'V^{-1}X)^{-1}X'V^{-1}Y = \hat{\beta}_W$$



## Moser 5.9 

### a
$$Y_{ij} = a + b_ix_j + E_{ij}$$
Let $Y = (Y_{11}-a, \dots, Y_{1n}-a, Y_{21}-a, \dots, Y_{2n}-a)' = (\tilde{Y_{1}}', \tilde{Y_2}')'$, we can rewrite the linear model in the matrix form. 

$$Y = (I_2 \otimes\tilde{x})\tilde{\beta} + (I_2\otimes I_n)\tilde{E}$$
where $\tilde{x} = (x_1, x_2, \dots, x_n)'$, $\tilde{\beta} = (b_1, b_2)'$ and $\tilde{E}\sim N_{2n}(0, \sigma^2I_{2n})$. We can identify the regression matrix $X = I_2\otimes \tilde{x}$. The least square estimator of $\tilde{\beta}$ is $\hat{\beta} = (X'X)^{-1}X'Y$. 

$$X'X = (I_2 \otimes \tilde{x}')(I_2\otimes \tilde{x}) = (\tilde{x}'\tilde{x})I_2 \Longrightarrow (X'X)^{-1} = (\tilde{x}'\tilde{x})^{-1}I_2$$
$$X'Y = (I_2\otimes \tilde{x}')Y = \begin{bmatrix}
\tilde{x}' & 0 \\ 
0 & \tilde{x}' \end{bmatrix}\begin{bmatrix}
\tilde{Y_1} \\
\tilde{Y_2} \end{bmatrix} = \begin{bmatrix}
\tilde{x}'\tilde{Y_1}\\
\tilde{x}'\tilde{Y_2} \end{bmatrix}$$

$$\hat{\beta} = \begin{bmatrix}
b_1 \\
b_2 \end{bmatrix} = (\tilde{x}'\tilde{x})^{-1}\begin{bmatrix}
\tilde{x}'\tilde{Y_1}\\
\tilde{x}'\tilde{Y_2} \end{bmatrix}$$

Let $t = (1, -1)'$, the BLUE of $b_1 - b_2 = t'\tilde{\beta}$ is $t'\hat{\beta}$

$$t'\hat{\beta} = (\tilde{x}'\tilde{x})^{-1}\left(\tilde{x}'\tilde{Y_1} - \tilde{x}'\tilde{Y_2} \right) = (\tilde{x}'\tilde{x})^{-1}\sum_{j=1}^nx_j(Y_{1j}-Y_{2j}) = \frac{\sum_{j=1}^nx_j(Y_{1j}-Y_{2j})}{\sum_{j=1}^nx_j^2}$$
The absence of constant $a$ is because $a\sum_{j=1}^nx_j = 0$. 


### b

We know that $\text{Cov}(\hat{\beta}) = \sigma^2(X'X)^{-1} = \sigma^2(\tilde{x}'\tilde{x})^{-1}I_2$. 

$$\text{Cov}(t'\hat{\beta}) = t'\text{Cov}(\hat{\beta})t = \frac{2\sigma^2}{\tilde{x}'\tilde{x}} = \frac{2\sigma^2}{\sum_{j=1}^nx_j^2}$$


## Moser 5.12

### a

Since $\mathbf{1}_a'X^* = 0$, each column of $X^*$ is centered and $\text{rank}(X^*) = p-1$. We know the inclusion of replicates is equivalent to averaging with respect to replicates to start with. Therefore, $A_1$, $A_2$ and $A_3 (A_{lof})$ take the form $(?? \otimes \bar{J_n})$. 

$$\beta_0: \quad A_1 = \bar{J_a}\otimes\bar{J_n} \quad \text{rank}(A_1) = 1$$
$$\beta: \quad A_2 = X^*(X^*{'}X^*)^{-1}X^*{'} \otimes \bar{J_n} \quad \text{rank}(A_2) = p-1$$
$$\text{Lack of Fit}: \quad A_{lof}=A_3 = \left(I_a - \bar{J_a} -X^*(X^*{'}X^*)^{-1}X^*{'} \right)\otimes \bar{J_n} \quad \text{rank}(A_3) = a-p > 1$$
$$\text{Pure Error}: \quad A_{pe} = A_4 = I_a\otimes C_n \quad \text{rank}(A_4) = a(n-1)$$
Moreover, 

$$\sum_{i=1}^4 A_i = I_a \otimes I_n = I_{an} \quad \sum_{i=1}^4 \text{rank}(A_i) = an$$

### b

Let $\mu = E[Y] = \beta_0(\mathbf{1}_a\otimes\mathbf{1}_n) + (X^*\otimes\mathbf{1}_n)\beta$ and $M = X^*(X^*{'}X^*)^{-1}X^*{'}$. Therefore, $M\bar{J} = 0$ and $M^2 = M$. The $A$-matrices satisfy the assumptions of Bhat's lemma with $A_iA_j = \delta_{ij}A_i$. Now, in order to apply Bhat's lemma

$$\text{Cov}(Y) = \Sigma = I_a \otimes \left(\sigma_1^2I_n + n\sigma_2 \bar{J_n} \right) = \sigma_1^2\sum_{i=1}^4A_i + n\sigma_2^2\sum_{i=1}^3A_i = \left(\sigma_1^2 + n\sigma_2^2 \right)\sum_{i=1}^3A_i + \sigma_1^2A_4$$
Therefore, by Bhat's lemma, 

$$Y'A_1Y \sim \left(\sigma_1^2 + n\sigma_2^2 \right)\chi_1^2(\delta_1)$$
$$\delta_1 = \left(\sigma_1^2 + n\sigma_2^2 \right)^{-1}\left[\beta_0^2(\mathbf{1}_a'\otimes\mathbf{1}_n')(\bar{J_a}\otimes\bar{J_n})(\mathbf{1}_a\otimes\mathbf{1}_n) \right] = \frac{an\beta_0^2}{\sigma_1^2 + n\sigma_2^2}$$
$$Y'A_2Y \sim \left(\sigma_1^2 + n\sigma_2^2 \right)\chi_{p-1}^2(\delta_2)$$
$$\delta_2 = \left(\sigma_1^2 + n\sigma_2^2 \right)^{-1}\left[\beta'(X^*{'}\otimes\mathbf{1}_n')(M\otimes\bar{J_n})(X^*\otimes\mathbf{1}_n)\beta \right] = \frac{n\beta'X^*{'}X^*\beta}{\sigma_1^2 + n\sigma_2^2}$$
$$Y'A_3Y \sim \left(\sigma_1^2 + n\sigma_2^2 \right)\chi_{a-p}^2(\delta_3) \quad \delta_3=0$$
$$\because \quad \mathbf{1}_a'(I_a - M - \bar{J_a})\mathbf{1}_a = 0 \quad \text{and} \quad X^*{'}(I_a - M - \bar{J_a})X^*=0$$
$$Y'A_3Y \sim n\sigma_2^2 \chi_{a(n-1)}^2(\delta_4) \quad \delta_4=0$$

### c

$$\Sigma = \sigma_1^2(I_a \otimes I_n) + n\sigma_2^2(I_a\otimes\bar{J_n})$$
$Y'A_iY$ and $Y'A_jY$ are independent if and only if $A_i\Sigma A_j=0$.

$$A_i\Sigma A_j = \sigma_1^2 A_iA_j + n\sigma_2^2A_i(I_a\otimes\bar{J_n})A_j$$
For $i\neq j$, the first term is always zero. The second term is zero when either $i=4$ or $j=4$. Moreover, for $i=1,2,3$ and $i\neq j$ the second term is zero because 
$$M\bar{J_a} = 0, \quad (I_a-M-\bar{J_a})\bar{J_a} = 0, \quad (I_a-M-\bar{J_a})M = 0$$
As a result, $Y'A_iY$ and $Y'A_jY$ are mutually independent since $\forall i,j \in \{1,2,3,4\}$ and $i \neq j$, $A_i\Sigma A_j = 0$.


### d

Observe that $\delta_2$ involves the vector $\beta$, which will show up in the noncentrality of the F-distribution. Define a statistic

$$F = \frac{Y'A_2Y/(p-1)}{Y'A_3Y/(a-p)} \sim F_{p-1, a-p}(\delta_2)$$
where 
$$\delta_2 = \frac{n\beta'X^*{'}X^*\beta}{\sigma_1^2 + n\sigma_2^2}$$
Under null hypothesis $H_0: \beta=0$, $F \sim F_{p-1, a-p}(0)$. 

