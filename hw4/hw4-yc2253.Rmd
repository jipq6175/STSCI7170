---
title: "STSCI 7170 Homework 4"
author: "Yen-Lin Chen (yc2253@cornell.edu)"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Problem 1

Let $X$ and $Y$ be two random variables and $a$ and $b$ be two scalars, we have 

$$\text{Var}(aX+bY) = a^2\text{Var}(X) + b^2\text{Var}(Y) + 2ab\text{Cov}(X, Y)$$
Therefore, given two unbiased estimates $\hat{\theta_1}$ and $\hat{\theta_2}$ and $\hat{\theta} = w\hat{\theta_1}+(1-w)\hat{\theta_2}$, the variance of the new estimate $\hat{\theta}$ is
$$\text{Var}(\hat{\theta}) = w^2\text{Var}(\hat{\theta_1}) + (1-w)^2\text{Var}(\hat{\theta_2}) + 2w(1-w)\text{Cov}(\hat{\theta_1},\hat{\theta_2}) \\ 
= w^2\sigma_1^2 + (1-w)^2\sigma_2^2 +2w(1-w)\sigma_{12}$$

We want to minimize $\text{Var}(\hat{\theta})$. 

$$\frac{d\text{Var}(\hat{\theta)}}{dw} = 2w\sigma_1^2-2(1-w)\sigma_2^2+2(1-2w)\sigma_{12} = 0$$

$$w = \frac{\sigma_2^2 - \sigma_{12}}{\sigma_1^2+\sigma_2^2-2\sigma_{12}}$$
If $\hat{\theta_1}$ and $\hat{\theta_2}$ are uncorrelated, i.e. $\sigma_{12}=0$, 

$$w = \frac{\sigma_2^2}{\sigma_1^2+\sigma_2^2}$$



## Problem 2


$y \sim N_n(\mu, \Sigma)$ where $\Sigma \in \mathbb{R}^{n\times n}$ and $\text{rank}(\Sigma) = m < n$. We can rewrite $\Sigma = Q\Lambda Q'$ where 
$$\Lambda = \begin{bmatrix} \Lambda_1 & 0_{m, n-m} \\
0_{n-m, m} & 0_{n-m, n-m} \end{bmatrix} \quad \Lambda_1 = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_m) \in \mathbb{R}^{m\times m} \quad \lambda_i > 0$$

The Moore-Penrose inverse of $\Sigma$ is $\Sigma^- = Q\Lambda^-Q'$ where

$$\Lambda^- = \begin{bmatrix} \Lambda_1^{-1} & 0_{m, n-m} \\
0_{n-m, m} & 0_{n-m, n-m} \end{bmatrix} \quad \Lambda_1^{-1} = \text{diag}(1/\lambda_1, 1/\lambda_2, \dots, 1/\lambda_m)$$

Define 

$$\sqrt{\Sigma^-} = Q\begin{bmatrix} \Lambda_1^{-1/2} & 0_{m, n-m} \\
0_{n-m, m} & 0_{n-m, n-m} \end{bmatrix}Q'$$

and $z = \sqrt{\Sigma^-}y \sim N_n(\sqrt{\Sigma^-}\mu, \sqrt{\Sigma^-}\Sigma\sqrt{\Sigma^-}) \sim N_n(\sqrt{\Sigma^-}\mu, K)$

$$K = \sqrt{\Sigma^-}\Sigma\sqrt{\Sigma^-} = Q\begin{bmatrix} I_m & 0_{m, n-m} \\
0_{n-m, m} & 0_{n-m, n-m} \end{bmatrix}Q' = \begin{bmatrix} I_m & 0_{m, n-m} \\
0_{n-m, m} & 0_{n-m, n-m} \end{bmatrix}$$

$K$ is idenpotent with rank $m$. Therefore, $z'Kz = z'z =  y'\sqrt{\Sigma^-}\sqrt{\Sigma^-}y = y'\Sigma^-y \sim \chi_m^2(\delta)$ where

$$\delta = (\sqrt{\Sigma^-}\mu)'K\sqrt{\Sigma^-}\mu = \mu'\Sigma^-\mu$$




## Problem 3

The model is 

$$Y_{ij} = \mu_0 + B_i + \tau_j + \gamma z_{ij} + E_{ij}$$
where $\mu_0, \tau_j, z_{ij}$ are fixed while $B_i\sim N(0, \sigma_b^2)$ and $E_{ij} \sim N(0, \sigma_e^2)$ are random. Re-write the model in the vector form.

$$Y = \mu_0(\mathbf{1}_b \otimes \mathbf{1}_t) + (I_b\otimes \mathbf{1}_t)\tilde{B} + (\mathbf{1}_b\otimes I_t)\tilde{\tau} + (I_b\otimes I_t)\tilde{E}$$
Let $Y_i = (Y_{i1}, Y_{i2}, \dots, Y_{it})'$, $z_i = (z_{i1}, z_{i2}, \dots, z_{it})'$ and $t\times t$ Helmert matrix $P$
$$P = \begin{bmatrix}
\frac{1}{\sqrt{t}} & \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & \dots & \frac{1}{\sqrt{t(t-1)}} \\
\frac{1}{\sqrt{t}} &-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & \dots & \frac{1}{\sqrt{t(t-1)}} \\
\frac{1}{\sqrt{t}} & 0 & -\frac{2}{\sqrt{6}} & \dots & \frac{1}{\sqrt{t(t-1)}} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\frac{1}{\sqrt{t}} & 0 & 0 & \dots & -\frac{t-1}{\sqrt{t(t-1)}} \end{bmatrix} = \begin{bmatrix} p_1 & p_2 & \dots & p_t \end{bmatrix}$$

the transformed responses are defined as $Y_{ij}^* = p_j'Y_i$. Note that $p_i'p_j = \delta_{ij}$

### 3a 

We can write $Y_i$ as the following. 

$$Y_i = \mu_0 \mathbf{1}_t + \gamma z_i + B_i\mathbf{1}_t + E_i$$
where $B_i \sim N(0, \sigma_b^2)$ and $E_i \sim N_t(0, \sigma_e^2I_t)$. Therefore, $Y_i \sim N_t\left[\mu_0 \mathbf{1}_t + \gamma z_i, \sigma_b^2\mathbf{1}_t\mathbf{1}_t' + \sigma_e^2I_t \right] \sim N_t\left[\mu_0\mathbf{1}_t + \gamma z_i, t\sigma_b^2\bar{J_t} + \sigma_e^2I_t \right]$. Let $\Sigma_i = t\sigma_b^2\bar{J_t} + \sigma_e^2I_t$ and $\Sigma_i$'s are identical for  $i=1,2,\dots,b$. 

$$j = 1: \quad \text{Var}(Y_{ij}^*) = p_1'\Sigma_i p_1 = \sigma_b^2\mathbf{1}_t'\bar{J_t}\mathbf{1}_t + \sigma_e^2 = t\sigma_b^2 + \sigma_e^2$$
$$j = 2,3,\dots, t:\quad \text{Var}(Y_{ij}^*) = p'_j\Sigma_ip_j = \sigma_e^2$$

### 3b

The estimates for $\gamma$ are 

$$\hat{\gamma_j} = \frac{\sum_{i=1}^b(z_{ij}^* - \bar{z_{.j}^*})Y_{ij}^*}{\sum_{i=1}^b(z_{ij}^* - \bar{z_{.j}^*})^2} = \frac{\sum_{i=1}^b(p_j'z_i - m)p_j'Y_i}{\sum_{i=1}^b(p_j'z_i-m)^2} \quad m = \frac{1}{b}\sum_{i=1}^b p_j'z_i$$
where agian, $Y_i \sim N_t\left[\mu_0\mathbf{1}_t + \gamma z_i, t\sigma_b^2\bar{J_t} + \sigma_e^2I_t \right]$. The only randomness lies in $Y_i$ because $p_j$'s are constants and $z_{ij}$'s are pre-determined. Note that the denominator is a scalar which can be simplified as follows. 

$$\sum_{i=1}^b(p_j'z_i-m)^2 = \sum_{i=1}^b(p_j'z_i)^2 - bm^2$$

First, we want to show that $\hat{\gamma_j}$ is unbiased. 

$$\text{E}[\hat{\gamma_j}] = \frac{\sum_{i=1}^b(p_j'z_i - m)p_j'\text{E}(Y_i)}{\sum_{i=1}^b(p_j'z_i-m)^2} = \frac{\sum_{i=1}^b(p_j'z_i - m)(\mu_0p_j'\mathbf{1}_t + \gamma p_j'z_i)}{\sum_{i=1}^b(p_j'z_i)^2 - bm^2}$$

We consider two cases where $j=1$ and $j=2,3, \dots, t$. 

For $j=1$

$$\text{E}[\hat{\gamma_1}] = \frac{\sum_{i=1}^b\left[\sqrt{t}\mu_0p_1'z_i - \sqrt{t}\mu_0m + \gamma(p_1'z_i)^2 - \gamma mp_1'z_i \right]}{\sum_{i=1}^b(p_1'z_i)^2 - bm^2} = \gamma \frac{\sum_{i=1}^b(p_1'z_i)^2 - bm^2}{\sum_{i=1}^b(p_1'z_i)^2 - bm^2} = \gamma
$$

For $j = 2,3, \dots, t$

$$\text{E}[\hat{\gamma_j}] = \frac{\sum_{i=1}^b\left[\gamma(p_j'z_i)^2 - \gamma mp_j'z_i \right]}{\sum_{i=1}^b(p_j'z_i)^2 - bm^2} = \gamma \frac{\sum_{i=1}^b(p_j'z_i)^2 - bm^2}{\sum_{i=1}^b(p_j'z_i)^2 - bm^2} = \gamma
$$

Therefore, $\hat{\gamma_j}$ is unbiased. We are now showing that $\hat{\gamma_j}$'s are independent. Notice that the denominator is just a scalar and the nominator is the sum of all the transformed $Y_i$'s, which happen to have identical covariance $\Sigma_i = t\sigma_b^2\bar{J_t} + \sigma_e^2I_t = \Sigma$ as in 3.a. Let the transformation matrices to be $T_j$ and $\hat{\gamma_j}$ and $\hat{\gamma_k}$ are independent if and only if $T_j\Sigma T_k'=0$

$$T_j = \frac{\sum_{i=1}^b(p_j'z_i - m)p_j'}{\sum_{i=1}^b(p_j'z_i-m)^2}$$
$T_j\Sigma T_k'=0 \iff \left[ \sum_{i=1}^b(p_j'z_i - m)p_j'\right] \Sigma \left[ \sum_{i=1}^b(p_k'z_i - m)p_k'\right]' =0$. Because the $p_j$'s are the columns of Helmert matrix, we have

$$p_j'p_k=0$$
$$p_j'\bar{J}p_k = 0$$
if $j\neq k$. Therefore, 

$$\left[ \sum_{i=1}^b(p_j'z_i - m)p_j'\right] \Sigma \left[ \sum_{l=1}^b(p_k'z_l - m)p_k'\right]' = \left[ \sum_{i=1}^b(p_j'z_i - m)p_j'\right] (t\sigma_b^2\bar{J_t} + \sigma_e^2I_t) \left[ \sum_{l=1}^bp_k(p_k'z_l - m)\right] = 0$$
for $j\neq k$ because of the properties of the Helmert matrix. Therefore, $\hat{\gamma_j}$ and $\hat{\gamma_k}$ are independent.


### 3c

Again, we discuss two cases with $j=1$ and $j=2,3,\dots, t$. 

For $j=1$

$$\text{Var}(\hat{\gamma_1}) = \frac{\sum_{i=1}^b(p_1'z_i - m)p_1'\Sigma p_1(p_1'z_i-m)}{\sum_{i=1}^b(p_1'z_i-m)^2} = \frac{\sum_{i=1}^b(p_1'z_i - m)^2(t\sigma_b^2+\sigma_e^2)}{\sum_{i=1}^b(p_1'z_i-m)^2} = t\sigma_b^2+\sigma_e^2$$

For $j=2,3,\dots, t$, 

$$\text{Var}(\hat{\gamma_j}) = \frac{\sum_{i=1}^b(p_j'z_i - m)p_j'\Sigma p_j(p_j'z_i-m)}{\sum_{i=1}^b(p_j'z_i-m)^2} = \sigma_e^2$$

The BLUE of $\gamma$ can be written as a convex linear combination of all the $\hat{\gamma_j}$'s.

$$\hat{\gamma}_{BLUE} = \sum_{j=1}^t a_j\hat{\gamma_j} \quad \sum_{j=1}^t a_j = 1 \quad \because \text{E}(\hat{\gamma}_{BLUE})=\gamma$$
And we should pick $a_j$'s such that $\hat{\gamma}_{BLUE}$ have the smallest variance. 

$$\text{Var}(\hat{\gamma}_{BLUE}) = \text{min}_{a_j}\left[ \sum_{j=1}^t a_j^2\text{Var}(\hat{\gamma_j}) \right] = \text{min}_{a_j}\left[ a_1^2(t\sigma_b^2+\sigma_e^2) + \sigma_e^2\sum_{j=2}^t a_j^2\right]$$

We can re-formulate this using $a = a_1$ and $b = a_j$ $\forall j = 2,3, \dots, t$. Therefore our goal is now 

$$\text{Var}(\hat{\gamma}_{BLUE}) = \text{min}_{a, b}\left[ a^2(t\sigma_b^2+\sigma_e^2) + (t-1)b^2\sigma_e^2 \right]$$
under the constraint of $a + (t-1)b = 1$. Substitute $b = \frac{1-a}{t-1}$ in the above and taking the derivative and set to $0$, we have

$$f(a) = a^2(t\sigma_b^2+\sigma_e^2) + \frac{1}{t-1}(1-a)^2\sigma_e^2$$
$$\frac{df}{da} = 0 = a(t\sigma_b^2+\sigma_e^2) - \frac{1-a}{t-1}\sigma_e^2 \iff a - (1-a)\frac{1-\rho}{t-1}=0$$
where $\rho = t\sigma_b^2/(t\sigma_b^2+\sigma_e^2)$. Solving the above equation, we end up with

$$a = \frac{1-\rho}{t-\rho} \quad b = \frac{1}{t-\rho}$$
$$\hat{\gamma}_{BLUE} = \left( \frac{1-\rho}{t-\rho}\right)\hat{\gamma_1} + \sum_{j=2}^t\left( \frac{1}{t-\rho}\right)\hat{\gamma_j}$$

### 3d

For $\rho \to 1$, 

$$\hat{\gamma}_{BLUE} \to \frac{1}{t-1}\sum_{j=2}^t\hat{\gamma_j}$$




## Moser 6.4

The $100(1-\gamma)\%$ confidence interval of $h'\beta$ is

$$h'\hat{\beta}\pm t_{n-p}^{\gamma/2}\sqrt{\frac{1}{n-p}\left[h'(X'X)^{-1}h\right]Y'\left[I-X(X'X)^{-1}X' \right]Y}$$


### a

Let $L$ be the length of the confidence interval, we have

$$L^2 = 4\left(t_{n-p}^{\gamma/2} \right)^2\left[\frac{h'(X'X)^{-1}h}{n-p}\right]Y'\left[I-X(X'X)^{-1}X' \right]Y$$
The only random vector is $Y \sim N_n(X\beta, \sigma^2I_n)$. Let $A_1 = X(X'X)^{-1}X'$ and $A_2 = I - X(X'X)^{-1}X'$, both $A_1$ and $A_2$ are idenpotent with rank $p$ and $n-p$ respectively. Moreover, $\sigma^2I_n = \sigma^2(A_1+A_2)$. By Bhat's lemma, we have 

$$Y'A_2Y \sim \sigma^2\chi_{n-p}^2(\delta_2) \quad \delta_2 = (X\beta)'A_2X\beta = 0$$
Therefore, 

$$L^2 \sim C\chi_{n-p}^2(0) \quad \text{where} \quad C=4\sigma^2\left(t_{n-p}^{\gamma/2} \right)^2\left[\frac{h'(X'X)^{-1}h}{n-p}\right]$$


### b

First, we find $\hat{\beta} = (X'X)^{-1}X'Y = X'Y = (5, 10, 15)' = (\hat{\beta_1}, \hat{\beta_2}, \hat{\beta_3})'$. We know that $100(1-\gamma)\%$ condifence interval of $h'\beta$ is

$$h'\hat{\beta}\pm t_{n-p}^{\gamma/2}\sqrt{\frac{1}{n-p}\left[h'(X'X)^{-1}h\right]Y'\left[I-X(X'X)^{-1}X' \right]Y}$$ 
$$ = h'\hat{\beta}\pm t_{17}^{\gamma/2}\sqrt{\frac{1}{17}h'h\left[Y'Y -(X'Y)'(X'Y) \right]} = h'\hat{\beta}\pm t_{17}^{\gamma/2}\sqrt{\frac{68}{17}h'h} = h'\hat{\beta}\pm 2\left(t_{17}^{\gamma/2}\right)\sqrt{h'h}$$
Now using $95\%$ condifence interval, $t_{17}^{\gamma/2}$ is the $97.5$ quantile of t-distribution with $17$ degrees of freedom. $t_{17}^{2.5} = 2.1098$. The $95\%$ condifence interval of $h'\beta$ is now

$$h'\hat{\beta}\pm 4.2196\sqrt{h'h}$$
The only thing we need to determine is $h$. 

$$h' = (1, 0, 0) \quad h'\beta = \beta_1 = \hat{\beta_1}\pm4.2196 \Longrightarrow \text{CI}_{95}(\beta_1) = [0.7804, 9.2196]$$
$$h' = (1, 1, 0) \quad h'\beta = \beta_1+\beta_2 = \hat{\beta_1}+\hat{\beta_2}\pm4.2196\sqrt{2} \Longrightarrow \text{CI}_{95}(\beta_1+\beta_2) = [9.0326, 20.9674]$$
$$h' = (1, 0, -1) \quad h'\beta = \beta_1-\beta_3 = \hat{\beta_1}-\hat{\beta_3}\pm4.2196\sqrt{2} \Longrightarrow \text{CI}_{95}(\beta_1-\beta_3) = [-15.9674, -4.0326]$$

## Moser 6.7

$$Y_{ij} = \mu_j + B_i + (BT)_{ij} \quad i = 1,2,\dots, n \quad j=1,2$$
$$Y = (\mathbf{1}_n \otimes I_2)\tilde{\mu} + (I_n \otimes \mathbf{1}_2)\tilde{B} + (I_n\otimes I_2)\tilde{BT}$$
where $\tilde{B} \sim N_n(0, \sigma_B^2I_n)$ and $\tilde{BT} \sim N_{2n}(0, \sigma_{BT}^2I_{2n})$. We can write $Y = X\beta +E$ where $X = \mathbf{1}_n \otimes I_2$, $\beta = \tilde{\mu} = (\mu_1, \mu_2)'$ and $E \sim N_{2n}(0, \Sigma)$. 

$$\Sigma = 2\sigma_B^2(I_n\otimes \bar{J}_2) + \sigma_{BT}^2(I_n\otimes I_2)$$
Define the A-matrices for fixed and random factors: 

$$A_1^f = \bar{J}_n \otimes \bar{J}_2 \quad \text{rank} = 1$$
$$A_1^r = C_n \otimes \bar{J}_2 \quad \text{rank} = n-1$$
$$A_2^f = \bar{J}_n \otimes C_2 \quad \text{rank} = 1$$
$$A_2^r = C_n \otimes C_2 \quad \text{rank} = n-1$$

$$\Sigma = 2\sigma_B^2(I_n\otimes \bar{J}_2) + \sigma_{BT}^2(I_n\otimes I_2) = (2\sigma_B^2+\sigma_{BT}^2)(A_1^f+A_1^r) + \sigma_{BT}^2(A_2^f+A_2^r)$$

### a

The maximum likelihood estimates are the following.

$$\hat{\beta} = (X'X)^{-1}X'Y = \frac{1}{n}(\mathbf{1}_n'\otimes I_2)Y = (\hat{\mu_1}, \hat{\mu_2})'$$
$$\hat{\mu_1} = \frac{1}{n}\sum_{i=1}^nY_{i1} \quad \hat{\mu_2} = \frac{1}{n}\sum_{i=1}^nY_{i2}$$
$$\hat{a_2} = \hat{\sigma}_{BT}^2 = \frac{Y'A_2^rY}{n} = \frac{1}{n} \sum_i \sum_j \left(Y_{ij} - \bar{Y}_{i.} - \bar{Y}_{.j} + \bar{Y}_{..} \right)^2$$
$$\hat{a_1} = 2\hat{\sigma}_{B}^2 + \hat{\sigma}_{BT}^2 = \frac{Y'A_1^rY}{n} = \frac{1}{n}\sum_i \left(\bar{Y}_{i.} - \bar{Y}_{..} \right)^2$$
$$\hat{\sigma}_{B}^2 = \frac{\hat{a_1} - \hat{a_2}}{2} = \frac{1}{2n}\left[\sum_i \left(\bar{Y}_{i.} - \bar{Y}_{..} \right)^2 - \sum_i \sum_j \left(Y_{ij} - \bar{Y}_{i.} - \bar{Y}_{.j} + \bar{Y}_{..} \right)^2 \right]$$

### b

Under $H_0: \mu_1=\mu_2$, the constraint is $H\beta=h=0$ where $H = (1, -1)$. Let $M = \frac{1}{n}(\mathbf{1}_n'\otimes I_2)$. $Y \sim N_{2n}(X\beta, \Sigma)$. Therefore 

$$\hat{\beta} \sim N_2(\beta, M\Sigma M') \Longrightarrow H\hat{\beta} = \hat{\mu_1}-\hat{\mu_2} \sim N(H\beta, HM\Sigma M'H') \sim N(\mu_1-\mu_2, HM\Sigma M'H')$$
$$HM\Sigma M'H' = \frac{1}{n^2}\left[(\mathbf{1}_n' \otimes (1,-1))2\sigma_B^2(I_n\otimes \bar{J}_2)(\mathbf{1}_n\otimes(1, -1)') + (\mathbf{1}_n' \otimes (1,-1))\sigma_{BT}^2(I_n\otimes I_2)(\mathbf{1}_n\otimes(1, -1)') \right]$$

$$HM\Sigma M'H' = \frac{1}{n^2}\left(2\sigma_B^2\times n \times 0 + 2n\sigma_{BT}^2 \right) = \frac{2\sigma_{BT}^2}{n}$$
Therefore, $H\hat{\beta} \sim N(\mu_1-\mu_2, 2\sigma_{BT}^2/n)$. We can construct the following statistics:

$$V = \frac{(H\hat{\beta})'[HM\Sigma M'H']^{-1}(H\hat{\beta})/1}{Y'(I-X(X'X)^{-1}X)Y/(2n-2)} = \frac{Y'(HM)'[HM\Sigma M'H']^{-1}HMY}{Y'(I-X(X'X)^{-1}X)Y/(2n-2)} \sim F_{1, 2n-2}(\delta)$$
We have 

$$(HM)'HM = \frac{1}{n^2}\left(\mathbf{1}_n\mathbf{1}_n' \otimes 2C_2 \right) = \frac{2}{n}(\bar{J}_n\otimes C_2)$$
$$I - X(X'X)^{-1}X' = I-XM = I - (\bar{J}_n \otimes I_2) = C_n \otimes I_2$$
Combining the avove 4 quantities, we have

$$V = \frac{Y'(HM)'[HM\Sigma M'H']^{-1}HMY}{Y'(I-X(X'X)^{-1}X)Y/(2n-2)} = \frac{Y'(\bar{J}_n\otimes C_2)Y/\sigma_{BT}^2}{Y'(C_n\otimes I_2)Y/(2n-2)} \sim F_{1, 2n-2}(\delta)$$
Therefore, 

$$\frac{Y'(\bar{J}_n\otimes C_2)Y}{Y'(C_n\otimes I_2)Y/(2n-2)} = \sigma_{BT}^2V \sim \sigma_{BT}^2F_{1, 2n-2}(\delta)$$
Under $H_0:\mu_1=\mu_2$, $\delta = 0$. 





