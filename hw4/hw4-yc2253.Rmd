---
title: "STSCI 7170 Homework 4"
author: "Yen-Lin Chen (yc2253@cornell.edu)"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Problem 1

Let $X$ and $Y$ be two random variables and $a$ and $b$ be two scalars, we have 

$$\text{Var}(aX+bY) = a^2\text{Var}(X) + b^2\text{Var}(Y) + 2ab\text{Cov}(X, Y)$$
Therefore, given two unbiased estimates $\hat{\theta_1}$ and $\hat{\theta_2}$ and $\hat{\theta} = w\hat{\theta_1}+(1-w)\hat{\theta_2}$, the variance of the new estimate $\hat{\theta}$ is
$$\text{Var}(\hat{\theta}) = w^2\text{Var}(\hat{\theta_1}) + (1-w)^2\text{Var}(\hat{\theta_2}) + 2w(1-w)\text{Cov}(\hat{\theta_1},\hat{\theta_2}) \\ 
= w^2\sigma_1^2 + (1-w)^2\sigma_2^2 +2w(1-w)\sigma_{12}$$

We want to minimize $\text{Var}(\hat{\theta})$. 

$$\frac{d\text{Var}(\hat{\theta)}}{dw} = 2w\sigma_1^2-2(1-w)\sigma_2^2+2(1-2w)\sigma_{12} = 0$$

$$w = \frac{\sigma_2^2 - \sigma_{12}}{\sigma_1^2+\sigma_2^2-2\sigma_{12}}$$
If $\hat{\theta_1}$ and $\hat{\theta_2}$ are uncorrelated, i.e. $\sigma_{12}=0$, 

$$w = \frac{\sigma_2^2}{\sigma_1^2+\sigma_2^2}$$



## Problem 2


$y \sim N_n(\mu, \Sigma)$ where $\Sigma \in \mathbb{R}^{n\times n}$ and $\text{rank}(\Sigma) = m < n$. We can rewrite $\Sigma = Q\Lambda Q'$ where 
$$\Lambda = \begin{bmatrix} \Lambda_1 & 0_{m, n-m} \\
0_{n-m, m} & 0_{n-m, n-m} \end{bmatrix} \quad \Lambda_1 = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_m) \in \mathbb{R}^{m\times m} \quad \lambda_i > 0$$

The Moore-Penrose inverse of $\Sigma$ is $\Sigma^- = Q\Lambda^-Q'$ where

$$\Lambda^- = \begin{bmatrix} \Lambda_1^{-1} & 0_{m, n-m} \\
0_{n-m, m} & 0_{n-m, n-m} \end{bmatrix} \quad \Lambda_1^{-1} = \text{diag}(1/\lambda_1, 1/\lambda_2, \dots, 1/\lambda_m)$$

Define 

$$\sqrt{\Sigma^-} = Q\begin{bmatrix} \Lambda_1^{-1/2} & 0_{m, n-m} \\
0_{n-m, m} & 0_{n-m, n-m} \end{bmatrix}Q'$$

and $z = \sqrt{\Sigma^-}y \sim N_n(\sqrt{\Sigma^-}\mu, \sqrt{\Sigma^-}\Sigma\sqrt{\Sigma^-}) \sim N_n(\sqrt{\Sigma^-}\mu, K)$

$$K = \sqrt{\Sigma^-}\Sigma\sqrt{\Sigma^-} = Q\begin{bmatrix} I_m & 0_{m, n-m} \\
0_{n-m, m} & 0_{n-m, n-m} \end{bmatrix}Q' = \begin{bmatrix} I_m & 0_{m, n-m} \\
0_{n-m, m} & 0_{n-m, n-m} \end{bmatrix}$$

$K$ is idenpotent with rank $m$. Therefore, $z'Kz = z'z =  y'\sqrt{\Sigma^-}\sqrt{\Sigma^-}y = y'\Sigma^-y \sim \chi_m^2(\delta)$ where

$$\delta = (\sqrt{\Sigma^-}\mu)'K\sqrt{\Sigma^-}\mu = \mu'\Sigma^-\mu$$




## Problem 3







## Moser 6.4

The $100(1-\gamma)\%$ confidence interval of $h'\beta$ is

$$h'\hat{\beta}\pm t_{n-p}^{\gamma/2}\sqrt{\frac{1}{n-p}\left[h'(X'X)^{-1}h\right]Y'\left[I-X(X'X)^{-1}X' \right]Y}$$


### a

Let $L$ be the length of the confidence interval, we have

$$L^2 = 4\left(t_{n-p}^{\gamma/2} \right)^2\left[\frac{h'(X'X)^{-1}h}{n-p}\right]Y'\left[I-X(X'X)^{-1}X' \right]Y$$
The only random vector is $Y \sim N_n(X\beta, \sigma^2I_n)$. Let $A_1 = X(X'X)^{-1}X'$ and $A_2 = I - X(X'X)^{-1}X'$, both $A_1$ and $A_2$ are idenpotent with rank $p$ and $n-p$ respectively. Moreover, $\sigma^2I_n = \sigma^2(A_1+A_2)$. By Bhat's lemma, we have 

$$Y'A_2Y \sim \sigma^2\chi_{n-p}^2(\delta_2) \quad \delta_2 = (X\beta)'A_2X\beta = 0$$
Therefore, 

$$L^2 \sim C\chi_{n-p}^2(0) \quad \text{where} \quad C=4\sigma^2\left(t_{n-p}^{\gamma/2} \right)^2\left[\frac{h'(X'X)^{-1}h}{n-p}\right]$$


### b

First, we find $\hat{\beta} = (X'X)^{-1}X'Y = X'Y = (5, 10, 15)' = (\hat{\beta_1}, \hat{\beta_2}, \hat{\beta_3})'$. We know that $100(1-\gamma)\%$ condifence interval of $h'\beta$ is

$$h'\hat{\beta}\pm t_{n-p}^{\gamma/2}\sqrt{\frac{1}{n-p}\left[h'(X'X)^{-1}h\right]Y'\left[I-X(X'X)^{-1}X' \right]Y}$$ 
$$ = h'\hat{\beta}\pm t_{17}^{\gamma/2}\sqrt{\frac{1}{17}h'h\left[Y'Y -(X'Y)'(X'Y) \right]} = h'\hat{\beta}\pm t_{17}^{\gamma/2}\sqrt{\frac{68}{17}h'h} = h'\hat{\beta}\pm 2\left(t_{17}^{\gamma/2}\right)\sqrt{h'h}$$
Now using $95\%$ condifence interval, $t_{17}^{\gamma/2}$ is the $97.5$ quantile of t-distribution with $17$ degrees of freedom. $t_{17}^{2.5} = 2.1098$. The $95\%$ condifence interval of $h'\beta$ is now

$$h'\hat{\beta}\pm 4.2196\sqrt{h'h}$$
The only thing we need to determine is $h$. 

$$h' = (1, 0, 0) \quad h'\beta = \beta_1 = \hat{\beta_1}\pm4.2196 \Longrightarrow \text{CI}_{95}(\beta_1) = [0.7804, 9.2196]$$
$$h' = (1, 1, 0) \quad h'\beta = \beta_1+\beta_2 = \hat{\beta_1}+\hat{\beta_2}\pm4.2196\sqrt{2} \Longrightarrow \text{CI}_{95}(\beta_1+\beta_2) = [9.0326, 20.9674]$$
$$h' = (1, 0, -1) \quad h'\beta = \beta_1-\beta_3 = \hat{\beta_1}-\hat{\beta_3}\pm4.2196\sqrt{2} \Longrightarrow \text{CI}_{95}(\beta_1-\beta_3) = [-15.9674, -4.0326]$$

## Moser 6.7

$$Y_{ij} = \mu_j + B_i + (BT)_{ij} \quad i = 1,2,\dots, n \quad j=1,2$$
$$Y = (\mathbf{1}_n \otimes I_2)\tilde{\mu} + (I_n \otimes \mathbf{1}_2)\tilde{B} + (I_n\otimes I_2)\tilde{BT}$$
where $\tilde{B} \sim N_n(0, \sigma_B^2I_n)$ and $\tilde{BT} \sim N_{2n}(0, \sigma_{BT}^2I_{2n})$. We can write $Y = X\beta +E$ where $X = \mathbf{1}_n \otimes I_2$, $\beta = \tilde{\mu} = (\mu_1, \mu_2)'$ and $E \sim N_{2n}(0, \Sigma)$. 

$$\Sigma = 2\sigma_B^2(I_n\otimes \bar{J}_2) + \sigma_{BT}^2(I_n\otimes I_2)$$
Define the A-matrices for fixed and random factors: 

$$A_1^f = \bar{J}_n \otimes \bar{J}_2 \quad \text{rank} = 1$$
$$A_1^r = C_n \otimes \bar{J}_2 \quad \text{rank} = n-1$$
$$A_2^f = \bar{J}_n \otimes C_2 \quad \text{rank} = 1$$
$$A_2^r = C_n \otimes C_2 \quad \text{rank} = n-1$$

$$\Sigma = 2\sigma_B^2(I_n\otimes \bar{J}_2) + \sigma_{BT}^2(I_n\otimes I_2) = (2\sigma_B^2+\sigma_{BT}^2)(A_1^f+A_1^r) + \sigma_{BT}^2(A_2^f+A_2^r)$$

### a

The maximum likelihood estimates are the following.

$$\hat{\beta} = (X'X)^{-1}X'Y = \frac{1}{n}(\mathbf{1}_n'\otimes I_2)Y = (\hat{\mu_1}, \hat{\mu_2})'$$
$$\hat{\mu_1} = \frac{1}{n}\sum_{i=1}^nY_{i1} \quad \hat{\mu_2} = \frac{1}{n}\sum_{i=1}^nY_{i2}$$
$$\hat{a_2} = \hat{\sigma}_{BT}^2 = \frac{Y'A_2^rY}{n} = \frac{1}{n} \sum_i \sum_j \left(Y_{ij} - \bar{Y}_{i.} - \bar{Y}_{.j} + \bar{Y}_{..} \right)^2$$
$$\hat{a_1} = 2\hat{\sigma}_{B}^2 + \hat{\sigma}_{BT}^2 = \frac{Y'A_1^rY}{n} = \frac{1}{n}\sum_i \left(\bar{Y}_{i.} - \bar{Y}_{..} \right)^2$$
$$\hat{\sigma}_{B}^2 = \frac{\hat{a_1} - \hat{a_2}}{2} = \frac{1}{2n}\left[\sum_i \left(\bar{Y}_{i.} - \bar{Y}_{..} \right)^2 - \sum_i \sum_j \left(Y_{ij} - \bar{Y}_{i.} - \bar{Y}_{.j} + \bar{Y}_{..} \right)^2 \right]$$

### b

Under $H_0: \mu_1=\mu_2$, the constraint is $H\beta=h=0$ where $H = (1, -1)$. Let $M = \frac{1}{n}(\mathbf{1}_n'\otimes I_2)$. $Y \sim N_{2n}(X\beta, \Sigma)$. Therefore 

$$\hat{\beta} \sim N_2(\beta, M\Sigma M') \Longrightarrow H\hat{\beta} = \hat{\mu_1}-\hat{\mu_2} \sim N(H\beta, HM\Sigma M'H') \sim N(\mu_1-\mu_2, HM\Sigma M'H')$$
$$HM\Sigma M'H' = \frac{1}{n^2}\left[(\mathbf{1}_n' \otimes (1,-1))2\sigma_B^2(I_n\otimes \bar{J}_2)(\mathbf{1}_n\otimes(1, -1)') + (\mathbf{1}_n' \otimes (1,-1))\sigma_{BT}^2(I_n\otimes I_2)(\mathbf{1}_n\otimes(1, -1)') \right]$$

$$HM\Sigma M'H' = \frac{1}{n^2}\left(2\sigma_B^2\times n \times 0 + 2n\sigma_{BT}^2 \right) = \frac{2\sigma_{BT}^2}{n}$$
Therefore, $H\hat{\beta} \sim N(\mu_1-\mu_2, 2\sigma_{BT}^2/n)$. We can construct the following statistics:

$$V = \frac{(H\hat{\beta})'[HM\Sigma M'H']^{-1}(H\hat{\beta})/1}{Y'(I-X(X'X)^{-1}X)Y/(2n-2)} = \frac{Y'(HM)'[HM\Sigma M'H']^{-1}HMY}{Y'(I-X(X'X)^{-1}X)Y/(2n-2)} \sim F_{1, 2n-2}(\delta)$$
We have 

$$(HM)'HM = \frac{1}{n^2}\left(\mathbf{1}_n\mathbf{1}_n' \otimes 2C_2 \right) = \frac{2}{n}(\bar{J}_n\otimes C_2)$$
$$I - X(X'X)^{-1}X' = I-XM = I - (\bar{J}_n \otimes I_2) = C_n \otimes I_2$$
Combining the avove 4 quantities, we have

$$V = \frac{Y'(HM)'[HM\Sigma M'H']^{-1}HMY}{Y'(I-X(X'X)^{-1}X)Y/(2n-2)} = \frac{Y'(\bar{J}_n\otimes C_2)Y/\sigma_{BT}^2}{Y'(C_n\otimes I_2)Y/(2n-2)} \sim F_{1, 2n-2}(\delta)$$
Therefore, 

$$\frac{Y'(\bar{J}_n\otimes C_2)Y}{Y'(C_n\otimes I_2)Y/(2n-2)} = \sigma_{BT}^2V \sim \sigma_{BT}^2F_{1, 2n-2}(\delta)$$
Under $H_0:\mu_1=\mu_2$, $\delta = 0$. 





